# src/llm_manager.py
import logging
import torch
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    pipeline,
    GenerationConfig
)
from langchain_community.llms import HuggingFacePipeline
import gc

logger = logging.getLogger(__name__)

class LLMManager:
    """G√®re les mod√®les l√©gers pour 4GB GPU"""
    
    def __init__(self, model_name: str = "microsoft/DialoGPT-medium"):
        self.model_name = model_name
        self.llm = None
        self.device = "cpu"
        self.is_initialized = False
        
    def initialize(self, **generation_kwargs):
        """
        Initialisation avec configuration corrig√©e pour √©viter la r√©p√©tition
        """
        try:
            logger.info(f"üöÄ Initialisation de {self.model_name} sur {self.device}")
            
            self._clean_memory()
            
            # Configuration SP√âCIFIQUE pour DialoGPT
            safe_kwargs = {
                "max_new_tokens": 150,  # R√©duit pour √©viter les r√©p√©titions
                "temperature": 0.8,     # Augment√© pour plus de cr√©ativit√©
                "top_p": 0.9,
                "top_k": 50,           # Ajout√© pour DialoGPT
                "do_sample": True,
                "repetition_penalty": 1.2,  # Augment√© pour √©viter les r√©p√©titions
                "pad_token_id": 50256,  # IMPORTANT: token de padding pour GPT
                "eos_token_id": 50256,  # Token de fin pour DialoGPT
            }
            safe_kwargs.update(generation_kwargs)
            
            logger.info("üì• Chargement du tokenizer...")
            tokenizer = AutoTokenizer.from_pretrained(self.model_name)
            
            # Configuration CRITIQUE pour DialoGPT
            if tokenizer.pad_token is None:
                tokenizer.pad_token = tokenizer.eos_token
            tokenizer.padding_side = "left"  # Important pour la g√©n√©ration
            
            logger.info("üì• Chargement du mod√®le sur CPU...")
            model = AutoModelForCausalLM.from_pretrained(
                self.model_name,
                device_map=None,
                torch_dtype=torch.float32,
                trust_remote_code=True
            )
            
            logger.info("‚öôÔ∏è Cr√©ation du pipeline avec configuration DialoGPT...")
            pipe = pipeline(
                "text-generation",
                model=model,
                tokenizer=tokenizer,
                max_new_tokens=safe_kwargs["max_new_tokens"],
                temperature=safe_kwargs["temperature"],
                top_p=safe_kwargs["top_p"],
                top_k=safe_kwargs["top_k"],
                repetition_penalty=safe_kwargs["repetition_penalty"],
                do_sample=safe_kwargs["do_sample"],
                pad_token_id=tokenizer.pad_token_id,
                eos_token_id=tokenizer.eos_token_id,
                device=-1,
                return_full_text=False,  # CRITIQUE: ne pas r√©p√©ter le prompt
            )
            
            self.llm = HuggingFacePipeline(pipeline=pipe)
            self.is_initialized = True
            
            logger.info(f"‚úÖ {self.model_name} initialis√© avec succ√®s")
            
            # Test avec un prompt qui force une r√©ponse diff√©rente
            test_prompt = "Utilisateur: Bonjour\nAssistant:"
            logger.info(f"üß™ Test avec prompt: {test_prompt}")
            test_response = self.generate_response(test_prompt)
            logger.info(f"üß™ R√©ponse de test: {test_response}")
            
        except Exception as e:
            logger.error(f"‚ùå Erreur lors de l'initialisation: {str(e)}", exc_info=True)
            self._try_alternative_model(**generation_kwargs)
    
    def _try_alternative_model(self, **generation_kwargs):
        """Tente un mod√®le alternatif si DialoGPT ne fonctionne pas"""
        alternative_models = [
            "microsoft/DialoGPT-large",  # Peut-√™tre mieux configur√©
            "distilgpt2",                # Plus basique mais fiable
            "gpt2",                      # Standard
        ]
        
        for alt_model in alternative_models:
            try:
                logger.info(f"üîÑ Tentative avec mod√®le alternatif: {alt_model}")
                self.model_name = alt_model
                self.llm = None
                self.is_initialized = False
                
                self.initialize(**generation_kwargs)
                if self.is_initialized:
                    logger.info(f"‚úÖ Mod√®le alternatif charg√©: {alt_model}")
                    return
                    
            except Exception as e:
                logger.warning(f"‚ùå {alt_model} a √©chou√©: {e}")
                continue
        
        logger.error("‚ùå Aucun mod√®le n'a pu √™tre charg√©")
        self.is_initialized = False
    
    def _clean_memory(self):
        """Nettoie la m√©moire"""
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        gc.collect()
    
    def generate_response(self, prompt: str) -> str:
        """G√©n√®re une r√©ponse avec gestion des r√©p√©titions"""
        if not self.is_initialized:
            return "‚ùå Le mod√®le n'est pas initialis√©"
        
        try:
            logger.info(f"üéØ G√©n√©ration pour prompt: {prompt[:100]}...")
            
            # Essayer invoke() d'abord
            response = self.llm.invoke(prompt)
            
            logger.info(f"üìù R√©ponse brute: {response}")
            
            cleaned_response = self._clean_response(response, prompt)
            logger.info(f"‚úÖ R√©ponse nettoy√©e: {cleaned_response[:100]}...")
            
            return cleaned_response
            
        except Exception as e:
            logger.error(f"‚ùå Erreur de g√©n√©ration: {str(e)}", exc_info=True)
            return f"‚ùå Erreur: {str(e)}"
    
    def _clean_response(self, response, original_prompt):
        """Nettoie la r√©ponse en supprimant les r√©p√©titions du prompt"""
        if isinstance(response, str):
            text = response
        else:
            text = str(response)
        
        logger.info(f"üßπ Nettoyage - R√©ponse originale: {text}")
        
        # Supprimer le prompt s'il est r√©p√©t√©
        if original_prompt in text:
            text = text.replace(original_prompt, "").strip()
        
        # Supprimer les pr√©fixes communs
        prefixes_to_remove = [
            "Utilisateur:", "User:", "Assistant:", "Bot:",
            "Human:", "AI:", "###", "**"
        ]
        
        for prefix in prefixes_to_remove:
            if text.startswith(prefix):
                text = text[len(prefix):].strip()
        
        # Nettoyer les espaces multiples
        text = ' '.join(text.split())
        
        # Si la r√©ponse est vide apr√®s nettoyage, retourner un message par d√©faut
        if not text or len(text) < 5:
            text = "Je n'ai pas pu g√©n√©rer une r√©ponse appropri√©e. Voici les extraits pertinents du document."
        
        logger.info(f"üßπ R√©ponse nettoy√©e: {text}")
        return text
    
    def create_technical_prompt(self, context: str, question: str, section_info: str = "") -> str:
        """Prompt optimis√© pour la documentation technique structur√©e"""
        logger.info(f"üìù Prompt cr√©√© - Format: conversation DialoGPT")
        return f"""En tant qu'expert technique, analyse les sections de documentation suivantes et r√©ponds √† la question.

        STRUCTURE DU DOCUMENT:
        {section_info}

        CONTENU TECHNIQUE:
        {context}

        QUESTION:
        {question}

        INSTRUCTIONS:
        1. R√©ponds en fran√ßais de mani√®re technique et structur√©e
        2. Utilise UNIQUEMENT les informations des sections fournies
        3. Mentionne la section pertinente quand c'est possible
        4. Si l'information est incompl√®te, indique quelles sections consulter
        5. Pour les questions d'installation/configuration, sois tr√®s pr√©cis
        6. Pour les erreurs, propose des solutions √©tape par √©tape

        R√âPONSE TECHNIQUE D√âTAILL√âE:"""
        
    
    def test_generation(self) -> str:
        """Teste la g√©n√©ration avec diff√©rents prompts"""
        if not self.is_initialized:
            return "‚ùå Mod√®le non initialis√©"
        
        try:
            # Test 1: Prompt simple
            test1 = self.generate_response("Utilisateur: Bonjour\nAssistant:")
            result1 = f"Test1: {test1[:50]}..." if test1 else "√âchec"
            
            # Test 2: Prompt avec contexte
            test2_prompt = self.create_technical_prompt(
                "L'IA transforme l'industrie.", 
                "Quel est l'impact de l'IA?"
            )
            test2 = self.generate_response(test2_prompt)
            result2 = f"Test2: {test2[:50]}..." if test2 else "√âchec"
            
            return f"‚úÖ Tests: {result1} | {result2}"
            
        except Exception as e:
            return f"‚ùå Test √©chou√©: {e}"
    
    def get_model_info(self) -> dict:
        """Retourne les informations du mod√®le"""
        return {
            "model": self.model_name,
            "device": self.device,
            "initialized": self.is_initialized
        }