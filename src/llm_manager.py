# src/llm_manager.py
import logging
import torch
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    pipeline,
    GenerationConfig
)
from langchain_community.llms import HuggingFacePipeline
import gc

logger = logging.getLogger(__name__)

class LLMManager:
    """G√®re les mod√®les l√©gers pour 4GB GPU"""
    
    def __init__(self, model_name: str = "microsoft/DialoGPT-medium"):
        self.model_name = model_name
        self.llm = None
        # Forcer le CPU pour 4GB GPU - plus stable
        self.device = "cpu"  # M√™me avec GPU, on utilise CPU pour stabilit√©
        self.is_initialized = False
        
    def initialize(self, **generation_kwargs):
        """
        Initialisation optimis√©e pour faible m√©moire
        """
        try:
            logger.info(f"üöÄ Initialisation de {self.model_name} sur {self.device}")
            
            self._clean_memory()
            
            # Param√®tres conservateurs pour faible m√©moire
            safe_kwargs = {
                "max_new_tokens": 200,
                "temperature": 0.7,
                "top_p": 0.9,
                "do_sample": True,
                "repetition_penalty": 1.1,
            }
            safe_kwargs.update(generation_kwargs)
            
            # Chargement du tokenizer
            logger.info("üì• Chargement du tokenizer...")
            tokenizer = AutoTokenizer.from_pretrained(self.model_name)
            
            # Configuration des tokens
            if tokenizer.pad_token is None:
                tokenizer.pad_token = tokenizer.eos_token
            
            # Chargement du mod√®le - SUR CPU pour stabilit√©
            logger.info("üì• Chargement du mod√®le sur CPU...")
            model = AutoModelForCausalLM.from_pretrained(
                self.model_name,
                device_map=None,  # Pas de device_map pour CPU
                torch_dtype=torch.float32,
                trust_remote_code=True
            )
            
            # Cr√©ation du pipeline
            logger.info("‚öôÔ∏è Cr√©ation du pipeline...")
            pipe = pipeline(
                "text-generation",
                model=model,
                tokenizer=tokenizer,
                max_new_tokens=safe_kwargs["max_new_tokens"],
                temperature=safe_kwargs["temperature"],
                top_p=safe_kwargs["top_p"],
                repetition_penalty=safe_kwargs["repetition_penalty"],
                do_sample=safe_kwargs["do_sample"],
                pad_token_id=tokenizer.eos_token_id,
                device=-1,  # -1 pour CPU
            )
            
            # Int√©gration avec LangChain
            self.llm = HuggingFacePipeline(pipeline=pipe)
            self.is_initialized = True
            
            logger.info(f"‚úÖ {self.model_name} initialis√© avec succ√®s sur {self.device}")
            
            # Test rapide
            test_result = self.test_generation()
            logger.info(f"üß™ Test de g√©n√©ration: {test_result}")
            
        except Exception as e:
            logger.error(f"‚ùå Erreur lors de l'initialisation: {str(e)}")
            self._try_ultra_light_model(**generation_kwargs)
    
    def _try_ultra_light_model(self, **generation_kwargs):
        """Tente un mod√®le ultra-l√©ger en dernier recours"""
        ultra_light_models = [
            "distilgpt2",           # ~500MB
            "gpt2",                 # ~600MB  
            "microsoft/DialoGPT-small",  # ~400MB
        ]
        
        for light_model in ultra_light_models:
            try:
                logger.info(f"üîÑ Tentative avec mod√®le ultra-l√©ger: {light_model}")
                self.model_name = light_model
                self.llm = None
                self.is_initialized = False
                
                self.initialize(**generation_kwargs)
                if self.is_initialized:
                    logger.info(f"‚úÖ Mod√®le ultra-l√©ger charg√©: {light_model}")
                    return
                    
            except Exception as e:
                logger.warning(f"‚ùå {light_model} a √©chou√©: {e}")
                continue
        
        logger.error("‚ùå Aucun mod√®le n'a pu √™tre charg√©")
        self.is_initialized = False
    
    def _clean_memory(self):
        """Nettoie la m√©moire"""
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        gc.collect()
    
    def generate_response(self, prompt: str) -> str:
        """G√©n√®re une r√©ponse"""
        if not self.is_initialized:
            return "‚ùå Le mod√®le n'est pas initialis√©. Veuillez r√©essayer."
        
        try:
            self._clean_memory()
            logger.info(f"üéØ G√©n√©ration pour: {prompt[:80]}...")
            
            # Utilisation simple et directe
            response = self.llm.invoke(prompt)
            
            cleaned_response = self._clean_response(response)
            logger.info(f"‚úÖ R√©ponse g√©n√©r√©e ({len(cleaned_response)} caract√®res)")
            
            return cleaned_response
            
        except Exception as e:
            logger.error(f"‚ùå Erreur de g√©n√©ration: {e}")
            return f"‚ùå Erreur lors de la g√©n√©ration: {str(e)}"
    
    def _clean_response(self, response):
        """Nettoie la r√©ponse"""
        if isinstance(response, str):
            text = response
        else:
            text = str(response)
        
        # Nettoyage basique
        text = text.strip()
        
        # Supprimer les √©ventuelles r√©p√©titions du prompt
        lines = text.split('\n')
        clean_lines = [line for line in lines if line.strip()]
        
        return ' '.join(clean_lines)
    
    def create_technical_prompt(self, context: str, question: str) -> str:
        """Cr√©e un prompt simple et efficace"""
        # Format simple pour les petits mod√®les
        return f"""Document: {context}

Question: {question}

R√©ponds en fran√ßais en utilisant uniquement le document. Si tu ne sais pas, dis-le.

R√©ponse:"""
    
    def test_generation(self) -> str:
        """Teste la g√©n√©ration"""
        if not self.is_initialized:
            return "‚ùå Mod√®le non initialis√©"
        
        try:
            response = self.generate_response("Bonjour, √ßa va?")
            if response and len(response) > 10:
                return f"‚úÖ Test r√©ussi: {response[:50]}..."
            else:
                return "‚ùå R√©ponse trop courte"
        except Exception as e:
            return f"‚ùå Test √©chou√©: {e}"
    
    def get_model_info(self) -> dict:
        """Retourne les informations du mod√®le"""
        return {
            "model": self.model_name,
            "device": self.device,
            "initialized": self.is_initialized,
            "memory_optimized": "Oui (4GB compatibilit√©)"
        }